# Deployment & Serving

[![npm version](https://img.shields.io/npm/v/deployment-serving.svg)](https://www.npmjs.com/package/deployment-serving)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> Deployment and serving infrastructure for LLM models and agents

## ğŸ“¦ Supported Platforms

- **vLLM** - High-throughput LLM serving
- **TGI** - Text Generation Inference (Hugging Face)
- **BentoML** - Model serving platform
- **Modal** - Serverless GPU infrastructure

## ğŸš€ Quick Start

```javascript
import { ModelServer, DeploymentManager, LoadBalancer } from '@agent-infra/deployment';

// Deploy a model
const server = new ModelServer({
  model: 'meta-llama/Llama-2-7b',
  provider: 'vllm',
  gpuCount: 1,
});

await server.start();
await server.deploy();

// Health check
const status = await server.health();
console.log(status);
```

## ğŸ“Š Features

- ğŸš€ One-click deployment
- âš¡ Auto-scaling
- ğŸ”„ Rolling updates
- ğŸ“ˆ Health monitoring
- ğŸ”’ SSL/TLS support

## ğŸ“ License

MIT License
